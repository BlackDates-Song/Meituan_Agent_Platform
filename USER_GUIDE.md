# ç¾å›¢å¤–å–æ¨èå¹³å° - å®Œæ•´ä½¿ç”¨æŒ‡å—

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å…‹éš†é¡¹ç›®
git clone [é¡¹ç›®åœ°å€]
cd Meituan

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
.\venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/Mac

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. é…ç½®ç¯å¢ƒå˜é‡

åˆ›å»º `.env` æ–‡ä»¶:

```bash
# LLMé…ç½®
LLM_PROVIDER=deepseek  # å¯é€‰: deepseek, qwen, openai
DEEPSEEK_API_KEY=sk-xxxxx
DEEPSEEK_BASE_URL=https://api.deepseek.com

# æˆ–ä½¿ç”¨Qwen
QWEN_API_KEY=sk-xxxxx
QWEN_BASE_URL=https://dashscope.aliyuncs.com/api/v1

# æˆ–ä½¿ç”¨OpenAI
OPENAI_API_KEY=sk-xxxxx
OPENAI_BASE_URL=https://api.openai.com/v1
```

### 3. æ•°æ®åˆå§‹åŒ–

```bash
# 1. åŠ è½½åŸå§‹æ•°æ®åˆ°SQLite
python scripts/load_data.py

# 2. ç‰¹å¾å·¥ç¨‹ä¸æ•°æ®é¢„å¤„ç†
python scripts/preprocess.py

# 3. åŠ è½½å›¾æ•°æ® (å¯é€‰ï¼Œç”¨äºNode2Vec)
python scripts/load_graph.py
```

### 4. å¯åŠ¨æœåŠ¡

**æ–¹å¼ä¸€: ä½¿ç”¨å¯åŠ¨è„šæœ¬ (æ¨è)**

```powershell
# å¯åŠ¨V2ç‰ˆæœ¬ (å®Œæ•´åŠŸèƒ½)
.\start_v2.ps1

# æˆ–å¯åŠ¨V1ç‰ˆæœ¬ (åŸºç¡€åŠŸèƒ½)
.\start.ps1
```

**æ–¹å¼äºŒ: æ‰‹åŠ¨å¯åŠ¨**

```bash
# åç«¯
cd backend
python app_v2.py

# å‰ç«¯ (æ–°å¼€ç»ˆç«¯)
cd frontend
python -m http.server 8080

# æµè§ˆå™¨è®¿é—®: http://localhost:8080/index_v2.html
```

---

## ğŸ“š åŠŸèƒ½ä½¿ç”¨æŒ‡å—

### 1. æ™ºèƒ½æ¨è

**åŸºç¡€æ¨è (ååŒè¿‡æ»¤)**

```python
import requests

response = requests.post("http://localhost:8000/api/recommend", json={
    "user_id": "1",
    "top_k": 10,
    "use_llm": False
})

print(response.json())
```

**LLMå¢å¼ºæ¨è**

```python
response = requests.post("http://localhost:8000/api/recommend", json={
    "user_id": "1",
    "top_k": 5,
    "use_llm": True  # å¯ç”¨LLMç”Ÿæˆæ¨èç†ç”±
})

# è¾“å‡ºåŒ…å«æ¨èç†ç”±
for rec in response.json()["recommendations"]:
    print(f"{rec['name']}: {rec.get('llm_reason', 'æ— ç†ç”±')}")
```

### 2. RAGè¯­ä¹‰æœç´¢

**å‰ç«¯ä½¿ç”¨**

1. æ‰“å¼€ http://localhost:8080/index_v2.html
2. åœ¨"RAGè¯­ä¹‰æœç´¢"æ¨¡å—è¾“å…¥æŸ¥è¯¢
3. æŸ¥çœ‹æ„å›¾è§£æç»“æœå’Œæœç´¢ç»“æœ

**APIè°ƒç”¨**

```python
response = requests.post("http://localhost:8000/api/rag/search", json={
    "query": "æ¨èé™„è¿‘è¯„åˆ†é«˜çš„å·èœé¦†",
    "top_k": 10
})

data = response.json()
print("æ„å›¾:", data["intent"])
print("ç»“æœæ•°:", data["total"])
for result in data["results"]:
    print(f"- {result['name']} (è¯„åˆ†: {result['score']})")
```

**æ„å»ºå‘é‡ç´¢å¼• (é¦–æ¬¡ä½¿ç”¨)**

```python
# æ„å»ºPOIå’ŒSPUå‘é‡ç´¢å¼•
response = requests.post("http://localhost:8000/api/vector/build")
print(response.json())  # æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡
```

### 3. Agentè¿è¥åˆ†æ

**è·å–å•†å®¶è¿è¥æŠ¥å‘Š**

```python
response = requests.post("http://localhost:8000/api/operation/analysis", json={
    "poi_id": "12345"
})

report = response.json()["report"]
print("å•†å®¶åç§°:", report["åŸºæœ¬ä¿¡æ¯"]["å•†å®¶åç§°"])
print("æœˆå‡é”€å”®é¢:", report["ç»è¥æ•°æ®"]["æœˆå‡é”€å”®é¢"])
print("è¿è¥å»ºè®®:")
for category, suggestions in report["è¿è¥å»ºè®®"].items():
    print(f"  {category}:")
    for s in suggestions:
        print(f"    - {s}")
```

**æŠ¥å‘Šç»“æ„**

```json
{
  "åŸºæœ¬ä¿¡æ¯": {
    "å•†å®¶åç§°": "æµ·åº•æç«é”…",
    "è¯„åˆ†": 4.7,
    "åˆ†ç±»": "ç«é”…",
    "åœ°å€": "æœé˜³åŒºxxx"
  },
  "ç»è¥æ•°æ®": {
    "æ€»è®¢å•é‡": 15234,
    "æœˆå‡é”€å”®é¢": 892345,
    "å®¢å•ä»·": 128,
    "æ´»è·ƒç”¨æˆ·æ•°": 8923
  },
  "çƒ­é—¨èœå“": [
    {"åç§°": "æ‹›ç‰Œæ¯›è‚š", "é”€é‡": 3245, "ä»·æ ¼": 38},
    ...
  ],
  "ç”¨æˆ·ç”»åƒ": {
    "ä¸»è¦å¹´é¾„æ®µ": "25-35å²",
    "ç”·å¥³æ¯”ä¾‹": "45:55",
    "æ¶ˆè´¹åå¥½": ["ç«é”…", "çƒ§çƒ¤"]
  },
  "ç«å“åˆ†æ": {
    "å‘¨è¾¹ç«å“æ•°": 8,
    "è¯„åˆ†æ’å": "ç¬¬2å",
    "ä»·æ ¼æ’å": "ä¸­ç­‰åä¸Š"
  },
  "è¿è¥å»ºè®®": {
    "èœå“ä¼˜åŒ–": ["å¢åŠ ä½ä»·å¥—é¤", "æ¨å‡ºå­£èŠ‚ç‰¹è‰²èœ"],
    "è¥é”€ç­–ç•¥": ["å·¥ä½œæ—¥åˆé—´ä¿ƒé”€", "ä¼šå‘˜ç§¯åˆ†æ´»åŠ¨"],
    "æœåŠ¡æ”¹è¿›": ["ç¼©çŸ­ç­‰å¾…æ—¶é—´", "ä¼˜åŒ–é…é€è·¯çº¿"]
  }
}
```

### 4. è¯„è®ºæ™ºèƒ½æ‘˜è¦

```python
response = requests.post("http://localhost:8000/api/comment/summary", json={
    "poi_id": "12345"
})

data = response.json()
print("å¹³å‡è¯„åˆ†:", data["average_score"])
print("æƒ…æ„Ÿåˆ†å¸ƒ:")
print(f"  æ­£é¢: {data['sentiment']['positive']}%")
print(f"  è´Ÿé¢: {data['sentiment']['negative']}%")
print("\nä¼˜ç‚¹:", ", ".join(data["positive_aspects"]))
print("ç¼ºç‚¹:", ", ".join(data["negative_aspects"]))
print("\nLLMæ‘˜è¦:", data["llm_summary"])
```

### 5. æ™ºèƒ½é—®ç­”

**é€šç”¨é—®ç­”**

```python
response = requests.post("http://localhost:8000/api/qa/answer", json={
    "question": "å¦‚ä½•æå‡å•†å®¶è¯„åˆ†ï¼Ÿ"
})

print(response.json()["answer"])
```

**å•†å®¶ç›¸å…³é—®ç­” (RAGæ£€ç´¢)**

```python
response = requests.post("http://localhost:8000/api/qa/answer", json={
    "question": "12345è¿™å®¶åº—çš„èœå“æ€ä¹ˆæ ·ï¼Ÿ",
    "poi_id": "12345"
})

print(response.json()["answer"])
```

---

## ğŸ”§ é«˜çº§é…ç½®

### ç¼“å­˜ç³»ç»Ÿ

**LLMç¼“å­˜**

```python
from backend.utils.cache import llm_cache

# æŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡
stats = llm_cache.get_stats()
print(f"å‘½ä¸­ç‡: {stats['hit_rate']:.2%}")
print(f"å‘½ä¸­æ¬¡æ•°: {stats['hits']}")
print(f"æœªå‘½ä¸­æ¬¡æ•°: {stats['misses']}")

# æ¸…ç©ºç¼“å­˜
llm_cache.clear()
```

**æŒä¹…åŒ–ç¼“å­˜**

```python
from backend.utils.cache import PersistentCache

cache = PersistentCache("my_cache", cache_dir="data/cache")
cache.set("key", {"data": "value"}, ttl=3600)
data = cache.get("key")
```

### æ€§èƒ½ç›‘æ§

**æŸ¥çœ‹APIæ€§èƒ½æŒ‡æ ‡**

```python
response = requests.get("http://localhost:8000/api/metrics")
metrics = response.json()

print("APIè°ƒç”¨ç»Ÿè®¡:")
for operation, stats in metrics["api_metrics"].items():
    print(f"  {operation}: å¹³å‡{stats['avg']:.3f}s (æœ€å¿«{stats['min']:.3f}s)")

print("\nLLMç¼“å­˜:")
print(f"  å‘½ä¸­ç‡: {metrics['cache']['llm_cache']['hit_rate']:.2%}")
```

**æŸ¥çœ‹æ—¥å¿—**

```bash
# æŸ¥çœ‹APIæ—¥å¿—
tail -f logs/api_v2_20240115.log

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
tail -f logs/api_v2_error_20240115.log
```

---

## ğŸ§ª æµ‹è¯•ä¸è°ƒè¯•

### è¿è¡ŒAPIæµ‹è¯•

```bash
# å®Œæ•´æµ‹è¯•æ‰€æœ‰ç«¯ç‚¹
python test_api.py

# æµ‹è¯•ç‰¹å®šåŠŸèƒ½
python -c "
import requests
resp = requests.post('http://localhost:8000/api/recommend', json={'user_id': '1'})
print(resp.json())
"
```

### å¸¸è§é—®é¢˜æ’æŸ¥

**é—®é¢˜1: æ•°æ®åº“è¿æ¥å¤±è´¥**

```bash
# æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨
ls data/db/meituan.db

# é‡æ–°åˆå§‹åŒ–æ•°æ®åº“
python scripts/load_data.py
```

**é—®é¢˜2: LLMè°ƒç”¨å¤±è´¥**

```bash
# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo $DEEPSEEK_API_KEY

# æµ‹è¯•APIè¿æ¥
curl -X POST https://api.deepseek.com/v1/chat/completions \
  -H "Authorization: Bearer $DEEPSEEK_API_KEY" \
  -d '{"model":"deepseek-chat","messages":[{"role":"user","content":"æµ‹è¯•"}]}'
```

**é—®é¢˜3: å‘é‡ç´¢å¼•æœªæ„å»º**

```bash
# æ‰‹åŠ¨æ„å»ºç´¢å¼•
curl -X POST http://localhost:8000/api/vector/build

# æˆ–åœ¨å‰ç«¯ç‚¹å‡»"æ„å»ºå‘é‡ç´¢å¼•"æŒ‰é’®
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. ç¼“å­˜ç­–ç•¥

```python
# æ¨èä½¿ç”¨æŒä¹…åŒ–ç¼“å­˜å­˜å‚¨å¸¸ç”¨æ•°æ®
from backend.utils.cache import PersistentCache

poi_cache = PersistentCache("poi_info", ttl=86400)  # 24å°æ—¶

def get_poi_info(poi_id):
    cached = poi_cache.get(poi_id)
    if cached:
        return cached
    
    # ä»æ•°æ®åº“æŸ¥è¯¢
    info = query_database(poi_id)
    poi_cache.set(poi_id, info)
    return info
```

### 2. æ‰¹é‡å¤„ç†

```python
from backend.utils.cache import BatchProcessor

processor = BatchProcessor(max_batch_size=50, timeout=1.0)

# æ‰¹é‡è·å–æ¨è
def batch_recommend(user_ids):
    def process_batch(batch):
        return [get_recommendations(uid) for uid in batch]
    
    return processor.process(user_ids, process_batch)
```

### 3. å¼‚æ­¥å¤„ç†

```python
# ä½¿ç”¨FastAPIçš„å¼‚æ­¥ç‰¹æ€§
@app.post("/api/recommend")
async def recommend(request: RecommendRequest):
    # å¹¶å‘æŸ¥è¯¢æ•°æ®åº“å’ŒLLM
    import asyncio
    
    db_task = asyncio.to_thread(query_database, request.user_id)
    llm_task = asyncio.to_thread(llm_generate, prompt) if request.use_llm else None
    
    db_result = await db_task
    llm_result = await llm_task if llm_task else None
    
    return merge_results(db_result, llm_result)
```

---

## ğŸ¨ å‰ç«¯è‡ªå®šä¹‰

### ä¿®æ”¹æ ·å¼

ç¼–è¾‘ `frontend/static/css/style_v2.css`:

```css
/* ä¿®æ”¹ä¸»é¢˜è‰² */
:root {
    --primary-color: #ff6600;  /* æ©™è‰² â†’ è‡ªå®šä¹‰é¢œè‰² */
    --secondary-color: #333333;
}

/* ä¿®æ”¹å¡ç‰‡æ ·å¼ */
.module-card {
    border-radius: 12px;  /* åœ†è§’ */
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
```

### æ·»åŠ æ–°æ¨¡å—

1. åœ¨ `index_v2.html` æ·»åŠ HTMLç»“æ„
2. åœ¨ `app_v2.js` æ·»åŠ Vueæ–¹æ³•
3. åœ¨ `app_v2.py` æ·»åŠ APIç«¯ç‚¹

```html
<!-- index_v2.html -->
<div class="module-card">
    <h3>æ–°åŠŸèƒ½æ¨¡å—</h3>
    <button @click="newFeature()">æ‰§è¡Œ</button>
    <div>{{ newResult }}</div>
</div>
```

```javascript
// app_v2.js
async newFeature() {
    const response = await axios.post('/api/new-feature', {...});
    this.newResult = response.data;
}
```

```python
# app_v2.py
@app.post("/api/new-feature")
async def new_feature(request: Request):
    return {"result": "success"}
```

---

## ğŸ“¦ éƒ¨ç½²æŒ‡å—

### Dockeréƒ¨ç½²

```dockerfile
# Dockerfile
FROM python:3.9

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
CMD ["python", "backend/app_v2.py"]
```

```bash
# æ„å»ºé•œåƒ
docker build -t meituan-platform .

# è¿è¡Œå®¹å™¨
docker run -p 8000:8000 -v $(pwd)/data:/app/data meituan-platform
```

### ç”Ÿäº§ç¯å¢ƒé…ç½®

```bash
# ä½¿ç”¨Gunicorn
pip install gunicorn
gunicorn backend.app_v2:app --workers 4 --bind 0.0.0.0:8000

# ä½¿ç”¨Nginxåå‘ä»£ç†
# /etc/nginx/sites-available/meituan
server {
    listen 80;
    server_name your-domain.com;
    
    location / {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
    }
}
```

---

## ğŸ” å®‰å…¨å»ºè®®

1. **APIå¯†é’¥ä¿æŠ¤**: æ°¸è¿œä¸è¦å°† `.env` æ–‡ä»¶æäº¤åˆ°Git
2. **é€Ÿç‡é™åˆ¶**: ç”Ÿäº§ç¯å¢ƒæ·»åŠ APIé™æµ
3. **è¾“å…¥éªŒè¯**: æ‰€æœ‰ç”¨æˆ·è¾“å…¥éƒ½åº”éªŒè¯
4. **HTTPS**: ç”Ÿäº§ç¯å¢ƒä½¿ç”¨SSLè¯ä¹¦

---

## ğŸ“ æ”¯æŒä¸åé¦ˆ

- é—®é¢˜æŠ¥å‘Š: [GitHub Issues]
- æŠ€æœ¯æ–‡æ¡£: [Wiki]
- é‚®ç®±è”ç³»: [your-email]

---

**ç¥ä½¿ç”¨æ„‰å¿«ï¼** ğŸ‰
